{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf470445-98fc-47d0-9845-eccedeca3b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Deep‑learning stack -------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871c975b-96c9-42ce-a3c6-8f0b3dc16c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    TRAIN_PATH      = \"train.parquet\"\n",
    "    TEST_PATH       = \"test.parquet\"\n",
    "    SUBMISSION_PATH = \"sample_submission.csv\"\n",
    "    \n",
    "    FEATURES = [\n",
    "        \"X863\", \"X856\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n",
    "        \"X415\", \"X345\", \"X855\", \"X174\", \"X302\", \"X178\", \"X168\", \"X612\", \"bid_qty\",\n",
    "        \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\", \"X888\", \"X421\", \"X333\",\n",
    "        \"X817\", \"X586\", \"X292\"\n",
    "    ]\n",
    "\n",
    "    # Subset fed to MLP (slightly different cols)\n",
    "    MLP_FEATURES = [\n",
    "        \"X863\", \"X856\", \"X598\", \"X862\", \"X385\", \"X603\", \"X860\", \"X674\", \"X415\", \"X345\", \"X855\", \"X174\", \"X302\",\n",
    "        \"X178\", \"X168\", \"X612\", \"X333\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\", 'log_volume',\n",
    "        'bid_ask_interaction', 'bid_buy_interaction', 'bid_sell_interaction', 'ask_buy_interaction',\n",
    "        'ask_sell_interaction']\n",
    "\n",
    "    LABEL_COLUMN     = \"label\"\n",
    "    N_FOLDS          = 3\n",
    "    RANDOM_STATE     = 42\n",
    "    OUTLIER_FRACTION = 0.001   # top 0.1 % residuals considered outliers\n",
    "    OUTLIER_STRATEGIES = [\"reduce\", \"remove\", \"double\", \"none\"]\n",
    "\n",
    "    lags = [1,3,5,10,20]\n",
    "# XGBoost base parameters ---------------------------------------------------\n",
    "XGB_PARAMS = {\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"colsample_bylevel\": 0.4778,\n",
    "    \"colsample_bynode\": 0.3628,\n",
    "    \"colsample_bytree\": 0.7107,\n",
    "    \"gamma\": 1.7095,\n",
    "    \"learning_rate\": 0.02213,\n",
    "    \"max_depth\": 20,\n",
    "    \"max_leaves\": 12,\n",
    "    \"min_child_weight\": 16,\n",
    "    \"n_estimators\": 1667,\n",
    "    \"subsample\": 0.06567,\n",
    "    \"reg_alpha\": 39.3524,\n",
    "    \"reg_lambda\": 75.4484,\n",
    "    \"verbosity\": 0,\n",
    "    \"random_state\": Config.RANDOM_STATE,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "LEARNERS = [\n",
    "    {\"name\": \"xgb\", \"Estimator\": XGBRegressor, \"params\": XGB_PARAMS}\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utility helpers (unchanged from modular version)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_activation_function(name):\n",
    "    if name is None:\n",
    "        return None\n",
    "    name = name.lower()\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    if name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    if name == \"sigmoid\":\n",
    "        return nn.Sigmoid()\n",
    "    raise ValueError(f\"Unsupported activation: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b167f954-b7ef-4489-ab2c-8a2998dc57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Checkpointer:\n",
    "    \"\"\"Simple best‑model saver\"\"\"\n",
    "    def __init__(self, path=\"best_model.pt\"):\n",
    "        self.path         = path\n",
    "        self.best_metric  = -np.inf\n",
    "\n",
    "    def __call__(self, metric, model):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(f\"✅  New best saved ({metric:.4f}) → {self.path}\")\n",
    "\n",
    "    def load(self, model):\n",
    "        model.load_state_dict(torch.load(self.path, map_location=device))\n",
    "        return model\n",
    "\n",
    "\n",
    "def get_dataloaders(X, y, hparams, shuffle=True):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    if y is not None:\n",
    "        y_t = torch.tensor(y, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        ds  = TensorDataset(X_t, y_t)\n",
    "    else:\n",
    "        ds  = TensorDataset(X_t)\n",
    "    return DataLoader(ds, batch_size=hparams[\"batch_size\"], shuffle=shuffle,\n",
    "                      generator=torch.Generator().manual_seed(hparams[\"seed\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c8816f-207d-4c84-b63c-56737275ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Feature Engineering\n",
    "# =========================\n",
    "def add_features(df):\n",
    "    df['bid_ask_interaction'] = df['bid_qty'] * df['ask_qty']\n",
    "    df['bid_buy_interaction'] = df['bid_qty'] * df['buy_qty']\n",
    "    df['bid_sell_interaction'] = df['bid_qty'] * df['sell_qty']\n",
    "    df['ask_buy_interaction'] = df['ask_qty'] * df['buy_qty']\n",
    "    df['ask_sell_interaction'] = df['ask_qty'] * df['sell_qty']\n",
    "\n",
    "    df['volume_weighted_sell'] = df['sell_qty'] * df['volume']\n",
    "    df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'])\n",
    "    df['selling_pressure'] = df['sell_qty'] / (df['volume'])\n",
    "    df['log_volume'] = np.log1p(df['volume'])\n",
    "\n",
    "    df['effective_spread_proxy'] = np.abs(df['buy_qty'] - df['sell_qty']) / (df['volume'])\n",
    "    df['bid_ask_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'])\n",
    "    df['order_flow_imbalance'] = (df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'])\n",
    "    df['liquidity_ratio'] = (df['bid_qty'] + df['ask_qty']) / (df['volume'])\n",
    "\n",
    "    FEATURES = [\n",
    "        \"X863\", \"X856\", \"X598\", \"X862\", \"X385\", \"X603\", \"X860\", \"X674\", \"X415\", \"X345\", \"X855\", \"X174\", \"X302\",\n",
    "        \"X178\", \"X168\", \"X612\", \"X333\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\", 'log_volume',\n",
    "        'bid_ask_interaction', 'bid_buy_interaction', 'bid_sell_interaction', 'ask_buy_interaction',\n",
    "        'ask_sell_interaction']\n",
    "    \n",
    "    lag_features = [f'{f}_lag_{l}' for f in FEATURES for l in Config.lags]\n",
    "    lead_features = [f'{f}_lead_{l}' for f in FEATURES for l in Config.lags]\n",
    "\n",
    "    for col in FEATURES:\n",
    "        for lag in Config.lags:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "            df[f'{col}_lead_{lag}'] = df[col].shift(-lag)\n",
    "\n",
    "    df = df.fillna(df.mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_time_decay_weights(n: int, decay: float = 0.9) -> np.ndarray:\n",
    "    positions = np.arange(n)\n",
    "    normalized = positions / (n - 1) if n > 1 else positions\n",
    "    weights = decay ** (1.0 - normalized)\n",
    "    return weights * n / weights.sum()\n",
    "\n",
    "def detect_outliers_and_adjust_weights(X, y, sample_weights, outlier_fraction=0.001, strategy=\"reduce\"):\n",
    "    \"\"\"\n",
    "    Detect outliers based on prediction residuals and adjust their weights.\n",
    "    \n",
    "    Strategies:\n",
    "    - \"reduce\": Current approach - reduce weights to 0.2-0.8x\n",
    "    - \"remove\": Set outlier weights to 0 (effectively removing them)\n",
    "    - \"double\": Double the weights of outliers\n",
    "    - \"none\": No adjustment\n",
    "    \"\"\"\n",
    "    if strategy == \"none\":\n",
    "        return sample_weights, np.zeros(len(y), dtype=bool)\n",
    "    \n",
    "    # Ensure we have at least some samples to detect outliers\n",
    "    n_samples = len(y)\n",
    "    if n_samples < 100:  # Not enough samples for meaningful outlier detection\n",
    "        print(f\"    Too few samples ({n_samples}) for outlier detection\")\n",
    "        return sample_weights, np.zeros(n_samples, dtype=bool)\n",
    "    \n",
    "    # Train a simple model to get residuals\n",
    "    rf = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, y, sample_weight=sample_weights)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    predictions = rf.predict(X)\n",
    "    residuals = np.abs(y - predictions)\n",
    "    \n",
    "    # Find threshold for top outlier_fraction\n",
    "    # Ensure we have at least 1 outlier\n",
    "    n_outliers = max(1, int(len(residuals) * outlier_fraction))\n",
    "    \n",
    "    # Sort residuals and get threshold\n",
    "    sorted_residuals = np.sort(residuals)\n",
    "    threshold = sorted_residuals[-n_outliers] if n_outliers <= len(residuals) else sorted_residuals[-1]\n",
    "    \n",
    "    # Create outlier mask\n",
    "    outlier_mask = residuals >= threshold\n",
    "    \n",
    "    # Ensure we have exactly n_outliers (handle ties at threshold)\n",
    "    if np.sum(outlier_mask) > n_outliers:\n",
    "        # If we have too many due to ties, randomly select to get exact number\n",
    "        outlier_indices = np.where(outlier_mask)[0]\n",
    "        np.random.seed(42)\n",
    "        selected_indices = np.random.choice(outlier_indices, n_outliers, replace=False)\n",
    "        outlier_mask = np.zeros(len(y), dtype=bool)\n",
    "        outlier_mask[selected_indices] = True\n",
    "    \n",
    "    # Adjust weights based on strategy\n",
    "    adjusted_weights = sample_weights.copy()\n",
    "    \n",
    "    if outlier_mask.any():\n",
    "        if strategy == \"reduce\":\n",
    "            # Original approach: reduce weights proportionally\n",
    "            outlier_residuals = residuals[outlier_mask]\n",
    "            min_outlier_res = outlier_residuals.min()\n",
    "            max_outlier_res = outlier_residuals.max()\n",
    "            \n",
    "            if max_outlier_res > min_outlier_res:\n",
    "                normalized_residuals = (outlier_residuals - min_outlier_res) / (max_outlier_res - min_outlier_res)\n",
    "            else:\n",
    "                normalized_residuals = np.ones_like(outlier_residuals)\n",
    "            \n",
    "            weight_factors = 0.8 - 0.6 * normalized_residuals\n",
    "            adjusted_weights[outlier_mask] *= weight_factors\n",
    "            \n",
    "        elif strategy == \"remove\":\n",
    "            # Set outlier weights to 0\n",
    "            adjusted_weights[outlier_mask] = 0\n",
    "            \n",
    "        elif strategy == \"double\":\n",
    "            # Double the weights of outliers\n",
    "            adjusted_weights[outlier_mask] *= 2.0\n",
    "        \n",
    "        print(f\"    Strategy '{strategy}': Adjusted {n_outliers} outliers ({outlier_fraction*100:.1f}% of data)\")\n",
    "    \n",
    "    return adjusted_weights, outlier_mask\n",
    "\n",
    "def load_data():\n",
    "    # Load data with all features available\n",
    "    all_features = list(set( Config.MLP_FEATURES))\n",
    "    RAW_FEATURES = [\n",
    "    \"X863\",\"X856\",\"X598\",\"X862\",\"X385\",\"X603\",\"X860\",\"X674\",\"X415\",\"X345\",\n",
    "    \"X855\",\"X174\",\"X302\",\"X178\",\"X168\",\"X612\",\"X333\",\n",
    "    \"bid_qty\",\"ask_qty\",\"buy_qty\",\"sell_qty\",\"volume\"\n",
    "    ]\n",
    "    train_df = pd.read_parquet(Config.TRAIN_PATH, columns=RAW_FEATURES + [Config.LABEL_COLUMN])\n",
    "    test_df = pd.read_parquet(Config.TEST_PATH, columns=RAW_FEATURES)\n",
    "    submission_df = pd.read_csv(Config.SUBMISSION_PATH)\n",
    "    print(f\"Loaded data - Train: {train_df.shape}, Test: {test_df.shape}, Submission: {submission_df.shape}\")\n",
    "\n",
    "    # Add features\n",
    "    train_df = add_features(train_df)\n",
    "    test_df = add_features(test_df)\n",
    "\n",
    "    return train_df.reset_index(drop=True), test_df.reset_index(drop=True), submission_df\n",
    "\n",
    "def get_model_slices(n_samples: int):\n",
    "    # Original 5 slices\n",
    "    base_slices = [\n",
    "        {\"name\": \"full_data\", \"cutoff\": 0, \"is_oldest\": False, \"outlier_adjusted\": False},\n",
    "        {\"name\": \"last_90pct\", \"cutoff\": int(0.10 * n_samples), \"is_oldest\": False, \"outlier_adjusted\": False},\n",
    "        {\"name\": \"last_85pct\", \"cutoff\": int(0.15 * n_samples), \"is_oldest\": False, \"outlier_adjusted\": False},\n",
    "        {\"name\": \"last_80pct\", \"cutoff\": int(0.20 * n_samples), \"is_oldest\": False, \"outlier_adjusted\": False},\n",
    "        {\"name\": \"oldest_25pct\", \"cutoff\": int(0.25 * n_samples), \"is_oldest\": True, \"outlier_adjusted\": False},\n",
    "    ]\n",
    "    \n",
    "    # Duplicate slices with outlier adjustment\n",
    "    outlier_adjusted_slices = []\n",
    "    for slice_info in base_slices:\n",
    "        adjusted_slice = slice_info.copy()\n",
    "        adjusted_slice[\"name\"] = f\"{slice_info['name']}_outlier_adj\"\n",
    "        adjusted_slice[\"outlier_adjusted\"] = True\n",
    "        outlier_adjusted_slices.append(adjusted_slice)\n",
    "    \n",
    "    return base_slices + outlier_adjusted_slices\n",
    "\n",
    "# =========================\n",
    "# Outlier Analysis Functions\n",
    "# =========================\n",
    "def analyze_outliers(train_df):\n",
    "    \"\"\"Analyze outliers in the training data\"\"\"\n",
    "    print(\"\\n=== Outlier Analysis ===\")\n",
    "    \n",
    "    X = train_df[Config.FEATURES].values\n",
    "    y = train_df[Config.LABEL_COLUMN].values\n",
    "    \n",
    "    # Get base weights\n",
    "    sample_weights = create_time_decay_weights(len(train_df))\n",
    "    \n",
    "    # Detect outliers\n",
    "    _, outlier_mask = detect_outliers_and_adjust_weights(\n",
    "        X, y, sample_weights, outlier_fraction=Config.OUTLIER_FRACTION, strategy=\"reduce\"\n",
    "    )\n",
    "    \n",
    "    # Analyze outlier characteristics\n",
    "    outlier_indices = np.where(outlier_mask)[0]\n",
    "    n_outliers = len(outlier_indices)\n",
    "    \n",
    "    print(f\"\\nTotal outliers detected: {n_outliers} ({n_outliers/len(train_df)*100:.2f}%)\")\n",
    "    \n",
    "    if n_outliers > 0:\n",
    "        # Statistical analysis\n",
    "        outlier_labels = y[outlier_mask]\n",
    "        normal_labels = y[~outlier_mask]\n",
    "        \n",
    "        print(f\"\\nLabel statistics:\")\n",
    "        print(f\"  Normal samples - Mean: {normal_labels.mean():.4f}, Std: {normal_labels.std():.4f}\")\n",
    "        print(f\"  Outlier samples - Mean: {outlier_labels.mean():.4f}, Std: {outlier_labels.std():.4f}\")\n",
    "        print(f\"  Label range - Normal: [{normal_labels.min():.4f}, {normal_labels.max():.4f}]\")\n",
    "        print(f\"  Label range - Outliers: [{outlier_labels.min():.4f}, {outlier_labels.max():.4f}]\")\n",
    "        \n",
    "        # Feature analysis for outliers\n",
    "        print(f\"\\nTop features with extreme values in outliers:\")\n",
    "        feature_names = Config.FEATURES[:20]  # Analyze first 20 features\n",
    "        outlier_features = train_df.iloc[outlier_indices][feature_names]\n",
    "        normal_features = train_df.iloc[~outlier_mask][feature_names]\n",
    "        \n",
    "        feature_diffs = []\n",
    "        for feat in feature_names:\n",
    "            outlier_mean = outlier_features[feat].mean()\n",
    "            normal_mean = normal_features[feat].mean()\n",
    "            if normal_mean != 0:\n",
    "                rel_diff = abs(outlier_mean - normal_mean) / abs(normal_mean)\n",
    "                feature_diffs.append((feat, rel_diff, outlier_mean, normal_mean))\n",
    "        \n",
    "        feature_diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "        for feat, diff, out_mean, norm_mean in feature_diffs[:10]:\n",
    "            print(f\"  {feat}: {diff*100:.1f}% difference (outlier: {out_mean:.4f}, normal: {norm_mean:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nNo outliers detected with current threshold. Consider adjusting outlier_fraction.\")\n",
    "    \n",
    "    return outlier_indices\n",
    "\n",
    "# =========================\n",
    "# XGBoost Training with Outlier Strategy Comparison\n",
    "# =========================\n",
    "def train_xgboost_with_outlier_comparison(train_df, test_df):\n",
    "    \"\"\"Train XGBoost with different outlier handling strategies and compare results\"\"\"\n",
    "    n_samples = len(train_df)\n",
    "    \n",
    "    # Store results for each strategy\n",
    "    strategy_results = {strategy: {\"oof_scores\": [], \"slice_scores\": {}} \n",
    "                       for strategy in Config.OUTLIER_STRATEGIES}\n",
    "    \n",
    "    # For final ensemble\n",
    "    best_strategy = \"reduce\"  # Default to current approach\n",
    "    best_score = -np.inf\n",
    "    best_oof_preds = None\n",
    "    best_test_preds = None\n",
    "    \n",
    "    for strategy in Config.OUTLIER_STRATEGIES:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing outlier strategy: {strategy.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get model slices for this strategy\n",
    "        model_slices = get_model_slices(n_samples)\n",
    "        \n",
    "        oof_preds = {\n",
    "            learner[\"name\"]: {s[\"name\"]: np.zeros(n_samples) for s in model_slices}\n",
    "            for learner in LEARNERS\n",
    "        }\n",
    "        test_preds = {\n",
    "            learner[\"name\"]: {s[\"name\"]: np.zeros(len(test_df)) for s in model_slices}\n",
    "            for learner in LEARNERS\n",
    "        }\n",
    "        \n",
    "        full_weights = create_time_decay_weights(n_samples)\n",
    "        kf = KFold(n_splits=Config.N_FOLDS, shuffle=False)\n",
    "        \n",
    "        for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df), start=1):\n",
    "            print(f\"\\n--- Fold {fold}/{Config.N_FOLDS} ---\")\n",
    "            X_valid = train_df.iloc[valid_idx][Config.FEATURES]\n",
    "            y_valid = train_df.iloc[valid_idx][Config.LABEL_COLUMN]\n",
    "            \n",
    "            for s in model_slices:\n",
    "                cutoff = s[\"cutoff\"]\n",
    "                slice_name = s[\"name\"]\n",
    "                is_oldest = s[\"is_oldest\"]\n",
    "                outlier_adjusted = s.get(\"outlier_adjusted\", False)\n",
    "                \n",
    "                if is_oldest:\n",
    "                    subset = train_df.iloc[:cutoff].reset_index(drop=True)\n",
    "                    rel_idx = train_idx[train_idx < cutoff]\n",
    "                    sw = np.ones(len(rel_idx))\n",
    "                else:\n",
    "                    subset = train_df.iloc[cutoff:].reset_index(drop=True)\n",
    "                    rel_idx = train_idx[train_idx >= cutoff] - cutoff\n",
    "                    sw = create_time_decay_weights(len(subset))[rel_idx] if cutoff > 0 else full_weights[train_idx]\n",
    "                \n",
    "                X_train = subset.iloc[rel_idx][Config.FEATURES]\n",
    "                y_train = subset.iloc[rel_idx][Config.LABEL_COLUMN]\n",
    "                \n",
    "                # Apply outlier strategy if this is an outlier-adjusted slice\n",
    "                if outlier_adjusted and len(X_train) > 100:\n",
    "                    sw, _ = detect_outliers_and_adjust_weights(\n",
    "                        X_train.values, \n",
    "                        y_train.values, \n",
    "                        sw, \n",
    "                        outlier_fraction=Config.OUTLIER_FRACTION,\n",
    "                        strategy=strategy\n",
    "                    )\n",
    "                \n",
    "                print(f\"  Training slice: {slice_name}, samples: {len(X_train)}\")\n",
    "                \n",
    "                for learner in LEARNERS:\n",
    "                    model = learner[\"Estimator\"](**learner[\"params\"])\n",
    "                    model.fit(X_train, y_train, sample_weight=sw, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "                    \n",
    "                    if is_oldest:\n",
    "                        oof_preds[learner[\"name\"]][slice_name][valid_idx] = model.predict(\n",
    "                            train_df.iloc[valid_idx][Config.FEATURES]\n",
    "                        )\n",
    "                    else:\n",
    "                        mask = valid_idx >= cutoff\n",
    "                        if mask.any():\n",
    "                            idxs = valid_idx[mask]\n",
    "                            oof_preds[learner[\"name\"]][slice_name][idxs] = model.predict(\n",
    "                                train_df.iloc[idxs][Config.FEATURES]\n",
    "                            )\n",
    "                        if cutoff > 0 and (~mask).any():\n",
    "                            base_slice_name = slice_name.replace(\"_outlier_adj\", \"\")\n",
    "                            if base_slice_name == slice_name:\n",
    "                                fallback_slice = \"full_data\"\n",
    "                            else:\n",
    "                                fallback_slice = \"full_data_outlier_adj\"\n",
    "                            oof_preds[learner[\"name\"]][slice_name][valid_idx[~mask]] = oof_preds[learner[\"name\"]][fallback_slice][\n",
    "                                valid_idx[~mask]\n",
    "                            ]\n",
    "                    \n",
    "                    test_preds[learner[\"name\"]][slice_name] += model.predict(test_df[Config.FEATURES])\n",
    "        \n",
    "        # Normalize test predictions\n",
    "        for learner_name in test_preds:\n",
    "            for slice_name in test_preds[learner_name]:\n",
    "                test_preds[learner_name][slice_name] /= Config.N_FOLDS\n",
    "        \n",
    "        # Evaluate this strategy\n",
    "        learner_name = 'xgb'\n",
    "        \n",
    "        # Weights for ensemble\n",
    "        weights = np.array([\n",
    "            1.0,   # full_data\n",
    "            1.0,   # last_90pct\n",
    "            1.0,   # last_85pct\n",
    "            1.0,   # last_80pct\n",
    "            0,  # oldest_25pct\n",
    "            0.9,   # full_data_outlier_adj\n",
    "            0.9,   # last_90pct_outlier_adj\n",
    "            0.9,   # last_85pct_outlier_adj\n",
    "            0.9,   # last_80pct_outlier_adj\n",
    "            0   # oldest_25pct_outlier_adj\n",
    "        ])\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        oof_weighted = pd.DataFrame(oof_preds[learner_name]).values @ weights\n",
    "        test_weighted = pd.DataFrame(test_preds[learner_name]).values @ weights\n",
    "        score_weighted = pearsonr(train_df[Config.LABEL_COLUMN], oof_weighted)[0]\n",
    "        \n",
    "        print(f\"\\n{strategy.upper()} Strategy - Weighted Ensemble Pearson: {score_weighted:.4f}\")\n",
    "        \n",
    "        # Store individual slice scores\n",
    "        slice_names = list(oof_preds[learner_name].keys())\n",
    "        for i, slice_name in enumerate(slice_names):\n",
    "            score = pearsonr(train_df[Config.LABEL_COLUMN], oof_preds[learner_name][slice_name])[0]\n",
    "            strategy_results[strategy][\"slice_scores\"][slice_name] = score\n",
    "            if \"outlier_adj\" in slice_name:\n",
    "                print(f\"  {slice_name}: {score:.4f} (weight: {weights[i]:.3f})\")\n",
    "        \n",
    "        strategy_results[strategy][\"oof_scores\"].append(score_weighted)\n",
    "        strategy_results[strategy][\"ensemble_score\"] = score_weighted\n",
    "        strategy_results[strategy][\"oof_preds\"] = oof_weighted\n",
    "        strategy_results[strategy][\"test_preds\"] = test_weighted\n",
    "        \n",
    "        # Track best strategy\n",
    "        if score_weighted > best_score:\n",
    "            best_score = score_weighted\n",
    "            best_strategy = strategy\n",
    "            best_oof_preds = oof_preds\n",
    "            best_test_preds = test_preds\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"OUTLIER STRATEGY COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for strategy in Config.OUTLIER_STRATEGIES:\n",
    "        score = strategy_results[strategy][\"ensemble_score\"]\n",
    "        print(f\"{strategy.upper()}: {score:.4f} {'← BEST' if strategy == best_strategy else ''}\")\n",
    "    \n",
    "    # Analyze differences\n",
    "    print(f\"\\nRelative performance vs 'reduce' strategy:\")\n",
    "    reduce_score = strategy_results[\"reduce\"][\"ensemble_score\"]\n",
    "    for strategy in Config.OUTLIER_STRATEGIES:\n",
    "        if strategy != \"reduce\":\n",
    "            score = strategy_results[strategy][\"ensemble_score\"]\n",
    "            diff = (score - reduce_score) / reduce_score * 100\n",
    "            print(f\"  {strategy}: {diff:+.2f}%\")\n",
    "    \n",
    "    return best_oof_preds, best_test_preds, model_slices, strategy_results, best_strategy\n",
    "\n",
    "# =========================\n",
    "# MLP Training (unchanged)\n",
    "# =========================\n",
    "def train_mlp(train_df, test_df):\n",
    "    print(\"\\n=== Training MLP Model ===\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    hparams = {\n",
    "        \"seed\": 42,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 1024 * 8 * 4,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"weight_decay\": 1e-3,\n",
    "        \"dropout_rate\": 0.6,\n",
    "        \"layers\": [len(Config.MLP_FEATURES), 256, 64, 1],\n",
    "        \"hidden_activation\": None,\n",
    "        \"activation\": \"relu\",\n",
    "        \"delta\": 5,\n",
    "        \"noise_factor\": 0.005\n",
    "    }\n",
    "    \n",
    "    set_seed(hparams[\"seed\"])\n",
    "    \n",
    "    # Prepare data for MLP\n",
    "    X_train_full = train_df[Config.MLP_FEATURES].values\n",
    "    y_train_full = train_df[Config.LABEL_COLUMN].values\n",
    "    \n",
    "    # Split for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2, shuffle=False, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(test_df[Config.MLP_FEATURES].values)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = get_dataloaders(X_train, y_train, hparams, shuffle=True)\n",
    "    val_loader = get_dataloaders(X_val, y_val, hparams, shuffle=False)\n",
    "    test_loader = get_dataloaders(X_test, None, hparams, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        layers=hparams[\"layers\"],\n",
    "        dropout_rate=hparams[\"dropout_rate\"],\n",
    "        activation=hparams[\"activation\"],\n",
    "        last_activation=hparams[\"hidden_activation\"],\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.HuberLoss(delta=hparams[\"delta\"], reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"], \n",
    "                          weight_decay=hparams[\"weight_decay\"])\n",
    "    \n",
    "    checkpointer = Checkpointer(path=\"best_mlp_model.pt\")\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Add noise for robustness\n",
    "            inputs = inputs + torch.randn_like(inputs) * hparams[\"noise_factor\"]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        running_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Training Loss: {running_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                trues.append(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        trues = np.concatenate(trues).flatten()\n",
    "        pearson_coef = pearsonr(preds, trues)[0]\n",
    "        print(f\"Validation Pearson Coef: {pearson_coef:.4f} | Loss: {val_loss:.4f}\")\n",
    "\n",
    "        checkpointer(pearson_coef, model)\n",
    "    \n",
    "    # Load best model and make predictions\n",
    "    model = checkpointer.load(model)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            inputs = inputs[0].to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions).flatten()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# =========================\n",
    "# Ensemble & Submission Functions\n",
    "# =========================\n",
    "def create_xgboost_submission(train_df, oof_preds, test_preds, submission_df, strategy=\"reduce\"):\n",
    "    learner_name = 'xgb'\n",
    "    \n",
    "    # Weights for 10 slices\n",
    "    weights = np.array([\n",
    "        1.0,   # full_data\n",
    "        1.0,   # last_90pct\n",
    "        1.0,   # last_85pct\n",
    "        1.0,   # last_80pct\n",
    "        0,  # oldest_25pct\n",
    "        0.9,   # full_data_outlier_adj\n",
    "        0.9,   # last_90pct_outlier_adj\n",
    "        0.9,   # last_85pct_outlier_adj\n",
    "        0.9,   # last_80pct_outlier_adj\n",
    "        0    # oldest_25pct_outlier_adj\n",
    "    ])\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    oof_weighted = pd.DataFrame(oof_preds[learner_name]).values @ weights\n",
    "    test_weighted = pd.DataFrame(test_preds[learner_name]).values @ weights\n",
    "    score_weighted = pearsonr(train_df[Config.LABEL_COLUMN], oof_weighted)[0]\n",
    "    print(f\"\\n{learner_name.upper()} Weighted Ensemble Pearson: {score_weighted:.4f}\")\n",
    "\n",
    "    # Print individual slice scores and weights for analysis\n",
    "    print(\"\\nIndividual slice OOF scores and weights:\")\n",
    "    slice_names = list(oof_preds[learner_name].keys())\n",
    "    for i, slice_name in enumerate(slice_names):\n",
    "        score = pearsonr(train_df[Config.LABEL_COLUMN], oof_preds[learner_name][slice_name])[0]\n",
    "        print(f\"  {slice_name}: {score:.4f} (weight: {weights[i]:.3f})\")\n",
    "\n",
    "    # Save XGBoost submission\n",
    "    xgb_submission = submission_df.copy()\n",
    "    xgb_submission[\"prediction\"] = test_weighted\n",
    "    xgb_submission.to_csv(f\"submission_xgboost_{strategy}.csv\", index=False)\n",
    "    print(f\"\\nSaved: submission_xgboost_{strategy}.csv\")\n",
    "    \n",
    "    return test_weighted, oof_weighted\n",
    "\n",
    "def create_ensemble_submission(xgb_predictions, mlp_predictions, submission_df, \n",
    "                             xgb_weight=0.9, mlp_weight=0.1, suffix=\"\"):\n",
    "    # Ensemble predictions\n",
    "    ensemble_predictions = (xgb_weight * xgb_predictions + \n",
    "                          mlp_weight * mlp_predictions)\n",
    "    \n",
    "    # Save ensemble submission\n",
    "    ensemble_submission = submission_df.copy()\n",
    "    ensemble_submission[\"prediction\"] = ensemble_predictions\n",
    "    filename = f\"submission_ensemble{suffix}.csv\"\n",
    "    ensemble_submission.to_csv(filename, index=False)\n",
    "    print(f\"\\nSaved: {filename} (XGBoost: {xgb_weight*100}%, MLP: {mlp_weight*100}%)\")\n",
    "    \n",
    "    return ensemble_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edad22aa-acf4-42e5-b306-faf226c87d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, submission_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "170e73dc-5bc3-46ee-a9e5-7b7d751effc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Outlier Analysis ===\n",
      "    Strategy 'reduce': Adjusted 525 outliers (0.1% of data)\n",
      "\n",
      "Total outliers detected: 525 (0.10%)\n",
      "\n",
      "Label statistics:\n",
      "  Normal samples - Mean: 0.0377, Std: 0.9746\n",
      "  Outlier samples - Mean: -1.5579, Std: 8.2826\n",
      "  Label range - Normal: [-24.4166, 20.7403]\n",
      "  Label range - Outliers: [-16.2274, 13.1532]\n",
      "\n",
      "Top features with extreme values in outliers:\n",
      "  X345: 830.6% difference (outlier: -0.1733, normal: 0.0237)\n",
      "  X598: 626.4% difference (outlier: -0.1587, normal: 0.0301)\n",
      "  X863: 409.8% difference (outlier: 0.0292, normal: -0.0094)\n",
      "  buy_qty: 316.4% difference (outlier: 546.7665, normal: 131.3119)\n",
      "  X385: 128.5% difference (outlier: -0.0201, normal: 0.0704)\n",
      "  X856: 109.8% difference (outlier: -0.0271, normal: -0.0129)\n",
      "  X178: 84.9% difference (outlier: 0.0050, normal: 0.0332)\n",
      "  X168: 83.7% difference (outlier: 0.0241, normal: 0.1475)\n",
      "  X174: 80.9% difference (outlier: 0.0281, normal: 0.1474)\n",
      "  X603: 79.2% difference (outlier: 0.2810, normal: 0.1569)\n"
     ]
    }
   ],
   "source": [
    "# Analyze outliers\n",
    "outlier_indices = analyze_outliers(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b20c98-e89f-46e5-9325-cf34846ca8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training XGBoost Models with Outlier Strategy Comparison ===\n",
      "\n",
      "==================================================\n",
      "Testing outlier strategy: REDUCE\n",
      "==================================================\n",
      "\n",
      "--- Fold 1/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 350591\n",
      "  Training slice: last_85pct, samples: 350591\n",
      "  Training slice: last_80pct, samples: 350591\n",
      "  Training slice: oldest_25pct, samples: 0\n",
      "    Strategy 'reduce': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "    Strategy 'reduce': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 350591\n",
      "    Strategy 'reduce': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 350591\n",
      "    Strategy 'reduce': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 350591\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 0\n",
      "\n",
      "--- Fold 2/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 298003\n",
      "  Training slice: last_85pct, samples: 271708\n",
      "  Training slice: last_80pct, samples: 245414\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "    Strategy 'reduce': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "    Strategy 'reduce': Adjusted 298 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298003\n",
      "    Strategy 'reduce': Adjusted 271 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271708\n",
      "    Strategy 'reduce': Adjusted 245 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245414\n",
      "    Strategy 'reduce': Adjusted 131 outliers (0.1% of data)\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "  Training slice: full_data, samples: 350592\n",
      "  Training slice: last_90pct, samples: 298004\n",
      "  Training slice: last_85pct, samples: 271709\n",
      "  Training slice: last_80pct, samples: 245415\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "    Strategy 'reduce': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350592\n",
      "    Strategy 'reduce': Adjusted 298 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298004\n",
      "    Strategy 'reduce': Adjusted 271 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271709\n",
      "    Strategy 'reduce': Adjusted 245 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245415\n",
      "    Strategy 'reduce': Adjusted 131 outliers (0.1% of data)\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "REDUCE Strategy - Weighted Ensemble Pearson: 0.1130\n",
      "  full_data_outlier_adj: 0.1125 (weight: 0.112)\n",
      "  last_90pct_outlier_adj: 0.1113 (weight: 0.112)\n",
      "  last_85pct_outlier_adj: 0.1050 (weight: 0.112)\n",
      "  last_80pct_outlier_adj: 0.1019 (weight: 0.112)\n",
      "  oldest_25pct_outlier_adj: 0.0672 (weight: 0.025)\n",
      "\n",
      "==================================================\n",
      "Testing outlier strategy: REMOVE\n",
      "==================================================\n",
      "\n",
      "--- Fold 1/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 350591\n",
      "  Training slice: last_85pct, samples: 350591\n",
      "  Training slice: last_80pct, samples: 350591\n",
      "  Training slice: oldest_25pct, samples: 0\n",
      "    Strategy 'remove': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "    Strategy 'remove': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 350591\n",
      "    Strategy 'remove': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 350591\n",
      "    Strategy 'remove': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 350591\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 0\n",
      "\n",
      "--- Fold 2/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 298003\n",
      "  Training slice: last_85pct, samples: 271708\n",
      "  Training slice: last_80pct, samples: 245414\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "    Strategy 'remove': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "    Strategy 'remove': Adjusted 298 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298003\n",
      "    Strategy 'remove': Adjusted 271 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271708\n",
      "    Strategy 'remove': Adjusted 245 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245414\n",
      "    Strategy 'remove': Adjusted 131 outliers (0.1% of data)\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "  Training slice: full_data, samples: 350592\n",
      "  Training slice: last_90pct, samples: 298004\n",
      "  Training slice: last_85pct, samples: 271709\n",
      "  Training slice: last_80pct, samples: 245415\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "    Strategy 'remove': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350592\n",
      "    Strategy 'remove': Adjusted 298 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298004\n",
      "    Strategy 'remove': Adjusted 271 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271709\n",
      "    Strategy 'remove': Adjusted 245 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245415\n",
      "    Strategy 'remove': Adjusted 131 outliers (0.1% of data)\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "REMOVE Strategy - Weighted Ensemble Pearson: 0.1117\n",
      "  full_data_outlier_adj: 0.1109 (weight: 0.112)\n",
      "  last_90pct_outlier_adj: 0.1101 (weight: 0.112)\n",
      "  last_85pct_outlier_adj: 0.1025 (weight: 0.112)\n",
      "  last_80pct_outlier_adj: 0.0945 (weight: 0.112)\n",
      "  oldest_25pct_outlier_adj: 0.0666 (weight: 0.025)\n",
      "\n",
      "==================================================\n",
      "Testing outlier strategy: DOUBLE\n",
      "==================================================\n",
      "\n",
      "--- Fold 1/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 350591\n",
      "  Training slice: last_85pct, samples: 350591\n",
      "  Training slice: last_80pct, samples: 350591\n",
      "  Training slice: oldest_25pct, samples: 0\n",
      "    Strategy 'double': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "    Strategy 'double': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 350591\n",
      "    Strategy 'double': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 350591\n",
      "    Strategy 'double': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 350591\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 0\n",
      "\n",
      "--- Fold 2/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 298003\n",
      "  Training slice: last_85pct, samples: 271708\n",
      "  Training slice: last_80pct, samples: 245414\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "    Strategy 'double': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "    Strategy 'double': Adjusted 298 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298003\n",
      "    Strategy 'double': Adjusted 271 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271708\n",
      "    Strategy 'double': Adjusted 245 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245414\n",
      "    Strategy 'double': Adjusted 131 outliers (0.1% of data)\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "  Training slice: full_data, samples: 350592\n",
      "  Training slice: last_90pct, samples: 298004\n",
      "  Training slice: last_85pct, samples: 271709\n",
      "  Training slice: last_80pct, samples: 245415\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "    Strategy 'double': Adjusted 350 outliers (0.1% of data)\n",
      "  Training slice: full_data_outlier_adj, samples: 350592\n",
      "    Strategy 'double': Adjusted 298 outliers (0.1% of data)\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298004\n",
      "    Strategy 'double': Adjusted 271 outliers (0.1% of data)\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271709\n",
      "    Strategy 'double': Adjusted 245 outliers (0.1% of data)\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245415\n",
      "    Strategy 'double': Adjusted 131 outliers (0.1% of data)\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "DOUBLE Strategy - Weighted Ensemble Pearson: 0.1144\n",
      "  full_data_outlier_adj: 0.1141 (weight: 0.112)\n",
      "  last_90pct_outlier_adj: 0.1145 (weight: 0.112)\n",
      "  last_85pct_outlier_adj: 0.1079 (weight: 0.112)\n",
      "  last_80pct_outlier_adj: 0.1037 (weight: 0.112)\n",
      "  oldest_25pct_outlier_adj: 0.0662 (weight: 0.025)\n",
      "\n",
      "==================================================\n",
      "Testing outlier strategy: NONE\n",
      "==================================================\n",
      "\n",
      "--- Fold 1/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 350591\n",
      "  Training slice: last_85pct, samples: 350591\n",
      "  Training slice: last_80pct, samples: 350591\n",
      "  Training slice: oldest_25pct, samples: 0\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "  Training slice: last_90pct_outlier_adj, samples: 350591\n",
      "  Training slice: last_85pct_outlier_adj, samples: 350591\n",
      "  Training slice: last_80pct_outlier_adj, samples: 350591\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 0\n",
      "\n",
      "--- Fold 2/3 ---\n",
      "  Training slice: full_data, samples: 350591\n",
      "  Training slice: last_90pct, samples: 298003\n",
      "  Training slice: last_85pct, samples: 271708\n",
      "  Training slice: last_80pct, samples: 245414\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "  Training slice: full_data_outlier_adj, samples: 350591\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298003\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271708\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245414\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "  Training slice: full_data, samples: 350592\n",
      "  Training slice: last_90pct, samples: 298004\n",
      "  Training slice: last_85pct, samples: 271709\n",
      "  Training slice: last_80pct, samples: 245415\n",
      "  Training slice: oldest_25pct, samples: 131471\n",
      "  Training slice: full_data_outlier_adj, samples: 350592\n",
      "  Training slice: last_90pct_outlier_adj, samples: 298004\n",
      "  Training slice: last_85pct_outlier_adj, samples: 271709\n",
      "  Training slice: last_80pct_outlier_adj, samples: 245415\n",
      "  Training slice: oldest_25pct_outlier_adj, samples: 131471\n",
      "\n",
      "NONE Strategy - Weighted Ensemble Pearson: 0.1130\n",
      "  full_data_outlier_adj: 0.1164 (weight: 0.112)\n",
      "  last_90pct_outlier_adj: 0.1096 (weight: 0.112)\n",
      "  last_85pct_outlier_adj: 0.1081 (weight: 0.112)\n",
      "  last_80pct_outlier_adj: 0.1001 (weight: 0.112)\n",
      "  oldest_25pct_outlier_adj: 0.0681 (weight: 0.025)\n",
      "\n",
      "==================================================\n",
      "OUTLIER STRATEGY COMPARISON SUMMARY\n",
      "==================================================\n",
      "REDUCE: 0.1130 \n",
      "REMOVE: 0.1117 \n",
      "DOUBLE: 0.1144 ← BEST\n",
      "NONE: 0.1130 \n",
      "\n",
      "Relative performance vs 'reduce' strategy:\n",
      "  remove: -1.19%\n",
      "  double: +1.24%\n",
      "  none: -0.04%\n",
      "\n",
      "XGB Weighted Ensemble Pearson: 0.1135\n",
      "\n",
      "Individual slice OOF scores and weights:\n",
      "  full_data: 0.1164 (weight: 0.132)\n",
      "  last_90pct: 0.1096 (weight: 0.132)\n",
      "  last_85pct: 0.1081 (weight: 0.132)\n",
      "  last_80pct: 0.1001 (weight: 0.132)\n",
      "  oldest_25pct: 0.0681 (weight: 0.000)\n",
      "  full_data_outlier_adj: 0.1141 (weight: 0.118)\n",
      "  last_90pct_outlier_adj: 0.1145 (weight: 0.118)\n",
      "  last_85pct_outlier_adj: 0.1079 (weight: 0.118)\n",
      "  last_80pct_outlier_adj: 0.1037 (weight: 0.118)\n",
      "  oldest_25pct_outlier_adj: 0.0662 (weight: 0.000)\n",
      "\n",
      "Saved: submission_xgboost_double.csv\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost with outlier comparison\n",
    "print(\"\\n=== Training XGBoost Models with Outlier Strategy Comparison ===\")\n",
    "best_oof_preds, best_test_preds, model_slices, strategy_results, best_strategy = \\\n",
    "    train_xgboost_with_outlier_comparison(train_df, test_df)\n",
    "\n",
    "# Create XGBoost submission with best strategy\n",
    "xgb_predictions, oof_weighted = create_xgboost_submission(\n",
    "    train_df, best_oof_preds, best_test_preds, submission_df, strategy=best_strategy\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "598e8652-eda0-4a43-ae19-1d0a3a322c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(train_df, test_df):\n",
    "    print(\"\\n=== Training MLP Model ===\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    hparams = {\n",
    "        \"seed\": 42,\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 1024 * 6,\n",
    "        \"learning_rate\": 6e-4,\n",
    "        \"weight_decay\": 6e-3,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"layers\": [len(Config.MLP_FEATURES), 128, 32, 1],\n",
    "        \"hidden_activation\": None,\n",
    "        \"activation\": \"relu\",\n",
    "        \"delta\": 5,\n",
    "        \"noise_factor\": 0.005\n",
    "    }\n",
    "    \n",
    "    set_seed(hparams[\"seed\"])\n",
    "    \n",
    "    # Prepare data for MLP\n",
    "    X_train_full = train_df[Config.MLP_FEATURES].values\n",
    "    y_train_full = train_df[Config.LABEL_COLUMN].values\n",
    "    \n",
    "    # Split for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2, shuffle=False, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(test_df[Config.MLP_FEATURES].values)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = get_dataloaders(X_train, y_train, hparams, shuffle=True)\n",
    "    val_loader = get_dataloaders(X_val, y_val, hparams, shuffle=False)\n",
    "    test_loader = get_dataloaders(X_test, None, hparams, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MLP(\n",
    "        layers=hparams[\"layers\"],\n",
    "        dropout_rate=hparams[\"dropout_rate\"],\n",
    "        activation=hparams[\"activation\"],\n",
    "        last_activation=hparams[\"hidden_activation\"],\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.HuberLoss(delta=hparams[\"delta\"], reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"], \n",
    "                          weight_decay=hparams[\"weight_decay\"])\n",
    "    \n",
    "    checkpointer = Checkpointer(path=\"best_mlp_model.pt\")\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = hparams[\"num_epochs\"]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Add noise for robustness\n",
    "            inputs = inputs + torch.randn_like(inputs) * hparams[\"noise_factor\"]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        running_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Training Loss: {running_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "                trues.append(targets.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        preds = np.concatenate(preds).flatten()\n",
    "        trues = np.concatenate(trues).flatten()\n",
    "        pearson_coef = pearsonr(preds, trues)[0]\n",
    "        print(f\"Validation Pearson Coef: {pearson_coef:.4f} | Loss: {val_loss:.4f}\")\n",
    "\n",
    "        checkpointer(pearson_coef, model)\n",
    "    \n",
    "    # Load best model and make predictions\n",
    "    model = checkpointer.load(model)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            inputs = inputs[0].to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions).flatten()\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers, d_model=128, nhead=8, num_layers=2,\n",
    "                 dropout_rate=0.5, activation=\"relu\", last_activation=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        # Transformer\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,\n",
    "            dropout=dropout_rate, activation=\"gelu\", batch_first=False\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        # MLP 头：首层用 d_model 而不是原 feature 数\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(d_model if i == 0 else layers[i], layers[i+1])\n",
    "             for i in range(len(layers)-1)]\n",
    "        )\n",
    "        self.act, self.last_act = get_activation_function(activation), get_activation_function(last_activation)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def _get_pos_emb(self, seq_len, device):\n",
    "        \"\"\"按当前长度生成正弦位置编码 (seq_len, 1, d_model)\"\"\"\n",
    "        pe = torch.zeros(seq_len, self.d_model, device=device)\n",
    "        position = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, device=device).float()\n",
    "                             * (-math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(1)            # (seq_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):                 # x : (batch, feature_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x_tok   = x.transpose(0, 1).unsqueeze(-1)        # (seq_len, batch, 1)\n",
    "        x_tok   = self.token_proj(x_tok)                 # (seq_len, batch, d_model)\n",
    "\n",
    "        pos_emb = self._get_pos_emb(seq_len, x_tok.device)\n",
    "        x_tok   = x_tok + pos_emb                        # 加位置\n",
    "        x_tok   = self.transformer(x_tok)                # (seq_len, batch, d_model)\n",
    "        h       = x_tok.mean(dim=0)                      # (batch, d_model)\n",
    "\n",
    "        # MLP 头\n",
    "        for layer in self.linears[:-1]:\n",
    "            h = self.dropout(self.act(layer(h)))\n",
    "        h = self.linears[-1](h)\n",
    "        if self.last_act:\n",
    "            h = self.last_act(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e1af4ba-2b90-4a86-ad1b-727c0af4cf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training MLP Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100%|██████████| 69/69 [00:11<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3000.5498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0251 | Loss: 3234.8294\n",
      "✅  New best saved (0.0251) → best_mlp_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 69/69 [00:11<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2980.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.1017 | Loss: 3197.2703\n",
      "✅  New best saved (0.1017) → best_mlp_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 69/69 [00:11<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2962.9233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.1204 | Loss: 3192.0588\n",
      "✅  New best saved (0.1204) → best_mlp_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 69/69 [00:11<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2952.3096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 13.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.1112 | Loss: 3218.2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 69/69 [00:11<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2929.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0828 | Loss: 3295.1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 69/69 [00:11<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2904.1715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0558 | Loss: 3513.4093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 69/69 [00:11<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2882.5355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0980 | Loss: 3564.3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 69/69 [00:11<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2853.2378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0851 | Loss: 4000.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 69/69 [00:11<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2825.9906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0903 | Loss: 3442.4820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 69/69 [00:11<00:00,  5.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2807.0844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:01<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Pearson Coef: 0.0593 | Loss: 6012.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 88/88 [00:04<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: submission_mlp.csv\n"
     ]
    }
   ],
   "source": [
    "# Train MLP model\n",
    "mlp_predictions = train_mlp(train_df, test_df)\n",
    "# mlp_oof, mlp_predictions = train_mlp_kfold(train_df, test_df)\n",
    "\n",
    "# Save MLP submission\n",
    "mlp_submission = submission_df.copy()\n",
    "mlp_submission[\"prediction\"] = mlp_predictions\n",
    "mlp_submission.to_csv(\"submission_mlp.csv\", index=False)\n",
    "print(\"\\nSaved: submission_mlp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c4db2-3828-4c54-858f-b0ec8e8ee4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble submission\n",
    "ensemble_predictions = create_ensemble_submission(\n",
    "    xgb_predictions, mlp_predictions, submission_df,\n",
    "    xgb_weight=0.9, mlp_weight=0.1, suffix=f\"_{best_strategy}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
